{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f235b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a677a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_randint():\n",
    "    while True:\n",
    "        yield randint(0, 9)\n",
    "\n",
    "\n",
    "def func(x: list) -> list:\n",
    "    y_0 = x[0]\n",
    "    y_all = [y_0]\n",
    "    for xi in x[1:]:\n",
    "        y_i = xi + y_0\n",
    "        if y_i >= 10:\n",
    "            y_i = y_i - 10\n",
    "        y_all.append(y_i)\n",
    "    return y_all\n",
    "\n",
    "\n",
    "n_examples = 5000\n",
    "s1 = [[next(get_randint()) for _ in range(25)] for _ in range(n_examples)]\n",
    "s2 = [[next(get_randint()) for _ in range(75)] for _ in range(n_examples)]\n",
    "s3 = [[next(get_randint()) for _ in range(125)] for _ in range(n_examples)]\n",
    "\n",
    "y1 = [func(x) for x in s1]\n",
    "y2 = [func(x) for x in s2]\n",
    "y3 = [func(x) for x in s3]\n",
    "\n",
    "s1, s2, s3 = torch.tensor(s1), torch.tensor(s2), torch.tensor(s3)\n",
    "y1, y2, y3 = torch.tensor(y1), torch.tensor(y2), torch.tensor(y3)\n",
    "\n",
    "# объединение последовательностей всех длин\n",
    "X = [s1, s2, s3]\n",
    "y = [y1, y2, y3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ec2f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, rnnClass, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden = rnnClass(input_size, hidden_size, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, X):   \n",
    "        o, _ = self.hidden(X)\n",
    "        output = self.output(o)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f8704a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, criterion, optimizer, batch=100):\n",
    "    for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            for seq_x, seq_y in zip(X, y): # достаем по паре последовательностей (s1, y1), (s2, y2), (s3, y3)\n",
    "                for i in range(int(len(seq_x) / batch)):\n",
    "                    \n",
    "                    inputs = seq_x[i * batch : (i + 1) * batch].unsqueeze(dim=2).to(torch.float32) # 100 x 25 x 1\n",
    "                    targets = seq_y[i * batch : (i + 1) * batch].unsqueeze(dim=2).flatten() # 2500 x 1  \n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs) # 100 x 25 x 10\n",
    "                    outputs = outputs.view(-1, output_size) # 2500 x 10\n",
    "                    \n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be82c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# так как не используются эмбеддинги, количество параметров (фич) каждого эл-та последовательности равно 1\n",
    "input_size = 1 \n",
    "hidden_size = 64\n",
    "output_size = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f45427d",
   "metadata": {},
   "source": [
    "### <проверка размерностей>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ce624b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1.shape=torch.Size([5000, 25]), y1.shape=torch.Size([5000, 25])\n",
      "s1.unsqueeze(dim=2).shape=torch.Size([5000, 25, 1]), y1.unsqueeze(dim=2).shape=torch.Size([5000, 25, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f'{s1.shape=}, {y1.shape=}')\n",
    "print(f'{s1.unsqueeze(dim=2).shape=}, {y1.unsqueeze(dim=2).shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1114915",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_check = s1.unsqueeze(dim=2).to(torch.float32)\n",
    "y1_check = y1.unsqueeze(dim=2).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc10b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 25, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = nn.RNN(1, 64, batch_first=True)\n",
    "hidden(s1_check[0:3])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef553c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 25, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = nn.Linear(64, 10)\n",
    "output(hidden(s1_check[0:3])[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb3778",
   "metadata": {},
   "source": [
    "### </проверка размерностей>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe702f1",
   "metadata": {},
   "source": [
    "# RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07db5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4d9db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RNN = NeuralNetwork(nn.RNN, input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e8fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_RNN.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd5b416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Loss: 1.9360119104385376\n",
      "Epoch 20/200, Loss: 2.2969093322753906\n",
      "Epoch 30/200, Loss: 1.8558143377304077\n",
      "Epoch 40/200, Loss: 1.795109510421753\n",
      "Epoch 50/200, Loss: 1.3494449853897095\n",
      "Epoch 60/200, Loss: 2.274125099182129\n",
      "Epoch 70/200, Loss: 1.9121074676513672\n",
      "Epoch 80/200, Loss: 1.7885265350341797\n",
      "Epoch 90/200, Loss: 1.6759361028671265\n",
      "Epoch 100/200, Loss: 1.0801721811294556\n",
      "Epoch 110/200, Loss: 0.989353358745575\n",
      "Epoch 120/200, Loss: 0.9236520528793335\n",
      "Epoch 130/200, Loss: 0.7453180551528931\n",
      "Epoch 140/200, Loss: 0.9558409452438354\n",
      "Epoch 150/200, Loss: 1.1549068689346313\n",
      "Epoch 160/200, Loss: 0.8634561896324158\n",
      "Epoch 170/200, Loss: 0.99730384349823\n",
      "Epoch 180/200, Loss: 0.9914222359657288\n",
      "Epoch 190/200, Loss: 0.7600827217102051\n",
      "Epoch 200/200, Loss: 0.7497560381889343\n"
     ]
    }
   ],
   "source": [
    "train_model(model_RNN, num_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ece777",
   "metadata": {},
   "source": [
    "### тестирование на обучающей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c9294ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8, 2, 8, 9, 4, 6, 8, 6, 7, 3, 8, 9, 6, 3, 7, 0, 5, 2, 1, 7, 4, 7, 3, 8,\n",
       "         3],\n",
       "        [9, 4, 7, 1, 5, 5, 5, 3, 6, 5, 8, 7, 8, 4, 3, 6, 8, 2, 4, 5, 5, 9, 1, 4,\n",
       "         1],\n",
       "        [6, 7, 4, 2, 1, 4, 5, 2, 0, 0, 1, 2, 1, 0, 1, 1, 9, 9, 0, 9, 9, 7, 6, 3,\n",
       "         1],\n",
       "        [0, 9, 9, 1, 7, 6, 9, 0, 2, 5, 8, 2, 1, 9, 1, 3, 3, 0, 9, 8, 3, 9, 8, 4,\n",
       "         5],\n",
       "        [7, 0, 2, 5, 6, 0, 5, 8, 0, 5, 6, 5, 9, 0, 1, 8, 2, 6, 3, 7, 7, 5, 6, 0,\n",
       "         6]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_test, y1_test = s1[0:5], y1[0:5]\n",
    "s1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6beb593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8, 0, 6, 7, 2, 4, 6, 4, 5, 1, 6, 7, 4, 1, 5, 7, 3, 0, 9, 5, 2, 5, 1, 6,\n",
       "         1],\n",
       "        [9, 2, 5, 9, 3, 3, 3, 1, 4, 3, 6, 5, 6, 2, 1, 4, 6, 0, 2, 3, 3, 7, 9, 2,\n",
       "         9],\n",
       "        [6, 3, 0, 8, 7, 0, 1, 8, 5, 5, 6, 8, 7, 6, 7, 7, 4, 4, 5, 4, 4, 2, 1, 8,\n",
       "         7],\n",
       "        [0, 9, 0, 2, 7, 7, 0, 1, 3, 6, 9, 3, 2, 0, 2, 4, 4, 1, 0, 8, 4, 0, 8, 5,\n",
       "         6],\n",
       "        [8, 7, 8, 3, 4, 7, 3, 6, 7, 3, 4, 3, 7, 7, 9, 6, 0, 4, 1, 5, 5, 3, 4, 7,\n",
       "         4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model_RNN(s1_test.unsqueeze(dim=2).to(torch.float32))\n",
    "    predicted = torch.argmax(output, dim=2)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e09868b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(predicted, y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab529c",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e60d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbbb2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = NeuralNetwork(nn.LSTM, input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfa0c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_LSTM.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "624b3dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.7779436111450195\n",
      "Epoch 20/100, Loss: 0.22038637101650238\n",
      "Epoch 30/100, Loss: 0.08809491246938705\n",
      "Epoch 40/100, Loss: 0.033889077603816986\n",
      "Epoch 50/100, Loss: 0.014261136762797832\n",
      "Epoch 60/100, Loss: 0.006758007686585188\n",
      "Epoch 70/100, Loss: 0.0033360205125063658\n",
      "Epoch 80/100, Loss: 0.0016286259051412344\n",
      "Epoch 90/100, Loss: 0.0012332035694271326\n",
      "Epoch 100/100, Loss: 0.0008903219713829458\n"
     ]
    }
   ],
   "source": [
    "train_model(model_LSTM, num_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "367beb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_test, y1_test = s1[0:5], y1[0:5]\n",
    "with torch.no_grad():\n",
    "    output = model_LSTM(s1_test.unsqueeze(dim=2).to(torch.float32))\n",
    "    predicted = torch.argmax(output, dim=2)\n",
    "torch.equal(predicted, y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed5d11",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3efb06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "463ac844",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU = NeuralNetwork(nn.GRU, input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95aa5603",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_GRU.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb679367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/70, Loss: 0.351041704416275\n",
      "Epoch 20/70, Loss: 0.069420225918293\n",
      "Epoch 30/70, Loss: 0.025917138904333115\n",
      "Epoch 40/70, Loss: 0.0116353714838624\n",
      "Epoch 50/70, Loss: 0.006219253409653902\n",
      "Epoch 60/70, Loss: 0.003778202459216118\n",
      "Epoch 70/70, Loss: 0.002216474385932088\n"
     ]
    }
   ],
   "source": [
    "train_model(model_GRU, num_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cf88874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_test, y1_test = s1[0:5], y1[0:5]\n",
    "with torch.no_grad():\n",
    "    output = model_GRU(s1_test.unsqueeze(dim=2).to(torch.float32))\n",
    "    predicted = torch.argmax(output, dim=2)\n",
    "torch.equal(predicted, y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484f05c3",
   "metadata": {},
   "source": [
    "# Проверка на тестовых данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7137b6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 100]), torch.Size([1000, 100]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тестирование моделей на новых тестовых датасетах 1000 х 100\n",
    "\n",
    "n_examples = 1000\n",
    "X_test = [[next(get_randint()) for _ in range(100)] for _ in range(n_examples)]\n",
    "y_test = [func(x) for x in X_test]\n",
    "\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "16d9e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(x, y):\n",
    "    x = x.numpy()\n",
    "    y = y.numpy()\n",
    "    assert x.size == y.size, 'Check dimensions!'\n",
    "    return (np.sum(x == y) / x.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5f8ade5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN accuracy: 0.6079\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model_RNN(X_test.unsqueeze(dim=2).to(torch.float32))\n",
    "    predicted = torch.argmax(output, dim=2)\n",
    "print('RNN accuracy:', calculate_accuracy(predicted, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b78b258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model_LSTM(X_test.unsqueeze(dim=2).to(torch.float32))\n",
    "    predicted = torch.argmax(output, dim=2)\n",
    "print('LSTM accuracy:', calculate_accuracy(predicted, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "851a1ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model_GRU(X_test.unsqueeze(dim=2).to(torch.float32))\n",
    "    predicted = torch.argmax(output, dim=2)\n",
    "print('GRU accuracy:', calculate_accuracy(predicted, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642288e5",
   "metadata": {},
   "source": [
    "# Выводы "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4659e5",
   "metadata": {},
   "source": [
    "Было проведено сравнение трех моделей рекуррентных сетей на задаче предсказывания последовательности чисел, зависящих от первого элемента последовательности.\n",
    "\n",
    "Модель RNN не смогла до конца выучить алгоритм последовательности по результатам 200 эпох. Итоговый loss 0.7497, Test_acc 0.6;\n",
    "\n",
    "Модель LSTM по результатам 100 эпох показала практически 100% точность. Итоговый loss 0.0008, Test_acc 1.0;\n",
    "\n",
    "Модель GRU по результатам 70 эпох также показала практически 100% точность. Итоговый loss 0.0022, Test_acc 1.0;\n",
    "\n",
    "В целом модели LSTM и GRU показали одинаково высокий результат\n",
    "и подходят для предсказания длинных последовательностей."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
